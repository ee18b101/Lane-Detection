#!/usr/bin/env python

# Copyright (c) 2019 Computer Vision Center (CVC) at the Universitat Autonoma de
# Barcelona (UAB).
#
# This work is licensed under the terms of the MIT license.
# For a copy, see <https://opensource.org/licenses/MIT>.

import glob
import os
import cv2
import sys
import time
import numpy as np
import cv2
from scipy.misc import imresize
from IPython.display import HTML
from keras.models import load_model

try:
    sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (
        sys.version_info.major,
        sys.version_info.minor,
        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])
except IndexError:
    pass

import carla

import random
import time

initial_time = time.time()
count1 = 0
count2 = 0

class Lanes():
    def __init__(self):
        self.recent_fit = []
        self.avg_fit = []


def road_lines(image):
    """ Takes in a road image, re-sizes for the model,
    predicts the lane to be drawn from the model in G color,
    recreates an RGB image of a lane and merges with the
    original road image.
    """

    # Get image ready for feeding into model
    small_img = imresize(image, (480, 360, 3))
    small_img = np.array(small_img)
    small_img = small_img[None,:,:,:]

    # Make prediction with neural network (un-normalize value by multiplying by 255)
    prediction = model.predict(small_img)[0] * 255

    # Add lane prediction to list for averaging
    lanes.recent_fit.append(prediction)
    # Only using last five for average
    if len(lanes.recent_fit) > 5:
        lanes.recent_fit = lanes.recent_fit[1:]

    # Calculate average detection
    lanes.avg_fit = np.mean(np.array([i for i in lanes.recent_fit]), axis = 0)

    # Generate fake R & B color dimensions, stack with G
    blanks = np.zeros_like(lanes.avg_fit).astype(np.uint8)
    lane_drawn = np.dstack((blanks, lanes.avg_fit, blanks))

    # Re-size to match the original image
    lane_image = imresize(lane_drawn, (360, 480, 3))
    # Merge the lane drawing onto the original image
    result = cv2.addWeighted(image, 1, lane_image, 1, 0)

    return result, lane_image


def process_img(image, cc, desegment = False):
    if desegment == False:
        image.save_to_disk(f'input/{530 + int((time.time() - initial_time)/10)}.png', cc)
        print('saved image')
    else:
        image.save_to_disk(f'flabergastered.png', carla.ColorConverter.CityScapesPalette)
        i = cv2.imread('flabergastered.png')
        lower_red = np.array([85,78.6,91.8])
        upper_red = np.array([85,78.6,91.8])
        mask = cv2.inRange(i, (48, 232, 155), (52, 236, 159))
        res = cv2.bitwise_and(i ,i, mask= mask)
        cv2.imwrite(f'output/{530 + int((time.time() - initial_time)/10)}.png', res)
        print('saved image')

    # Where to save the output video

    # result, lane_image = road_lines(image)
    # cv2.imshow('result', result)
    # cv2.waitKey(0)
    # cv2.imshow('lane_image', lane_image)
    # cv2.waitKey(0)



def main():
    actor_list = []


    model = load_model('full_CNN_model.h5')
    # Create lanes object
    lanes = Lanes()

    # In this tutorial script, we are going to add a vehicle to the simulation
    # and let it drive in autopilot. We will also create a camera attached to
    # that vehicle, and save all the images generated by the camera to disk.

    try:
        # First of all, we need to create the client that will send the requests
        # to the simulator. Here we'll assume the simulator is accepting
        # requests in the localhost at port 2000.
        client = carla.Client('localhost', 2000)
        client.set_timeout(5.0)

        # Once we have a client we can retrieve the world that is currently
        # running.
        world = client.get_world()
        initial_time = time.time()
        # The world contains the list blueprints that we can use for adding new
        # actors into the simulation.
        blueprint_library = world.get_blueprint_library()

        # Now let's filter all the blueprints of type 'vehicle' and choose one
        # at random.
        model_3 = blueprint_library.filter("model3")[0]

        # A blueprint contains the list of attributes that define a vehicle's
        # instance, we can read them and modify some of them. For instance,
        # let's randomize its color.
        # if bp.has_attribute('color'):
        #     color = random.choice(bp.get_attribute('color').recommended_values)
        #     bp.set_attribute('color', color)

        # Now we need to give an initial transform to the vehicle. We choose a
        # random transform from the list of recommended spawn points of the map.
        transform = random.choice(world.get_map().get_spawn_points())

        # So let's tell the world to spawn the vehicle.
        vehicle = world.spawn_actor(model_3, transform)

        # It is important to note that the actors we create won't be destroyed
        # unless we call their "destroy" function. If we fail to call "destroy"
        # they will stay in the simulation even after we quit the Python script.
        # For that reason, we are storing all the actors we create so we can
        # destroy them afterwards.
        actor_list.append(vehicle)
        print('created %s' % vehicle.type_id)

        # Let's put the vehicle to drive around.
        vehicle.set_autopilot(True)

        # Let's add now a "depth" camera attached to the vehicle. Note that the
        # transform we give here is now relative to the vehicle.

        rgb_cam = blueprint_library.find('sensor.camera.rgb')
        rgb_cam.set_attribute("image_size_x", f"{480}")
        rgb_cam.set_attribute("image_size_y", f"{360}")
        rgb_cam.set_attribute("fov", f"110")

        transform = carla.Transform(carla.Location(x=2.5, z=0.7))
        sensor = world.spawn_actor(rgb_cam, transform, attach_to=vehicle)
        actor_list.append(sensor)

        segment_cam = blueprint_library.find('sensor.camera.semantic_segmentation')
        segment_cam.set_attribute("image_size_x", f"{480}")
        segment_cam.set_attribute("image_size_y", f"{360}")
        segment_cam.set_attribute("fov", f"110")

        transform = carla.Transform(carla.Location(x=2.5, z=0.7))
        sensor_segment = world.spawn_actor(segment_cam, transform, attach_to=vehicle)
        actor_list.append(sensor_segment)
        # camera_bp = blueprint_library.find('sensor.camera.depth')
        # camera_transform = carla.Transform(carla.Location(x=1.5, z=2.4))
        # camera = world.spawn_actor(camera_bp, camera_transform, attach_to=vehicle)
        # actor_list.append(camera)
        print('created %s' % sensor.type_id)
        print('created %s' % sensor_segment.type_id)

        # Now we register the function that will be called each time the sensor
        # receives an image. In this example we are saving the image to disk
        # converting the pixels to gray-scale.
        cc = carla.ColorConverter.Raw
        sensor.listen(lambda image: process_img(image, cc))
        sensor_segment.listen(lambda image: process_img(image, cc, True))

        time.sleep(100000)

    finally:

        print('destroying actors')
        for actor in actor_list:
            actor.destroy()
        print('done.')


if __name__ == '__main__':
    frame_count = 0
    main()
